{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN_X2kubSwu6",
        "outputId": "77edb735-ba6d-4dd0-8b95-44a64fea2091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 10 22:31:47 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   35C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aceDqgGA5Fen"
      },
      "source": [
        "## Required Imports and Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dAEXia5Eluh",
        "outputId": "7ab335ea-eea9-468d-d0a6-098356240f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.3.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.6\n"
          ]
        }
      ],
      "source": [
        "%pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n71555TJElui"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "\n",
        "from six.moves import xrange\n",
        "\n",
        "import umap\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_HseY32Eluj"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sif_cUv_Eluk"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "cYY3xgRhkH53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "SdnAtpwDSIs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayetJ24VEluk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ae8749-26a9-490c-d86e-441d4d8ba86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43478747.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.CIFAR10(root=\"data\", train=True, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))\n",
        "\n",
        "validation_data = datasets.CIFAR10(root=\"data\", train=False, download=True,\n",
        "                                  transform=transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "                                  ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_-qvH-N36jD"
      },
      "outputs": [],
      "source": [
        "# training_data = datasets.Flowers102(root=\"data\", split=\"train\", download=True,\n",
        "#                                     transform=transforms.Compose([\n",
        "#                                       transforms.RandomCrop(256),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "#                                   ]))\n",
        "# validation_data = datasets.Flowers102(root=\"data\", split=\"val\", download=True,\n",
        "#                                       transform=transforms.Compose([\n",
        "#                                       transforms.CenterCrop(256),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "#                                   ]))\n",
        "# testing_data = datasets.Flowers102(root=\"data\", split=\"test\", download=True,\n",
        "#                                    transform=transforms.Compose([\n",
        "#                                       transforms.CenterCrop(256),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
        "#                                   ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQLj9l7Calj0"
      },
      "outputs": [],
      "source": [
        "# data = datasets.MNIST(root=\"data\", train=True, download=True,\n",
        "#                                   transform=transforms.Compose([\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5,), (1.0,))\n",
        "#                                   ]))\n",
        "\n",
        "\n",
        "# training_data, validation_data = torch.utils.data.random_split(data, [50000, 10000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training_data = datasets.CelebA(\"data\", split=\"train\", download=True, transform=transform)\n",
        "# validation_data = datasets.CelebA(\"data\", split=\"valid\", download=True, transform=transform)\n",
        "# testing_data = datasets.CelebA(\"data\", split=\"test\", download=True, transform=transform)"
      ],
      "metadata": {
        "id": "-byZNeJnjtRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0-srjox_mfq",
        "outputId": "d1f269cb-7428-44b8-eaac-7d96e84974b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# size of the training data\n",
        "len(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "RXgYSFdZ5I8D",
        "outputId": "c75338c8-8b52-427e-fbee-2f5b63c6db7c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(1.0, 1.0, 1.0))\n",
              "           )"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0Nn-GL3bdF3"
      },
      "outputs": [],
      "source": [
        "# universal experiment set-up\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 1000\n",
        "\n",
        "num_channels = 3 # 1 for grayscale images 3 for RGB images\n",
        "num_hiddens = 128\n",
        "num_residual_hiddens = 32\n",
        "num_residual_layers = 2\n",
        "\n",
        "embedding_dim = 16\n",
        "num_embeddings = 512\n",
        "\n",
        "commitment_cost = 0.25\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "disc_start = int(0.8 * (len(training_data) / batch_size) * num_epochs) if num_epochs >= 5000 else sys.maxsize\n",
        "\n",
        "log_interval = 100\n",
        "\n",
        "debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhxUGpMubaPL"
      },
      "outputs": [],
      "source": [
        "# define dataloaders\n",
        "training_loader = DataLoader(training_data,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True,\n",
        "                             pin_memory=True)\n",
        "\n",
        "validation_loader = DataLoader(validation_data,\n",
        "                               batch_size=32,\n",
        "                               shuffle=False,\n",
        "                               pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oAux3baElul",
        "outputId": "8e9c8d4e-c245-4bab-df2e-ccf36eeddccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset variance: 0.06132998690009117\n"
          ]
        }
      ],
      "source": [
        "# compute dataset variance\n",
        "data_variance = 0.\n",
        "\n",
        "for x, _ in training_loader:\n",
        "    data_variance += x.var()\n",
        "data_variance /= len(training_loader)\n",
        "\n",
        "print(f'Dataset variance: {data_variance}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5IkSvUP5nXr"
      },
      "source": [
        "## VQ-VAE\n",
        "\n",
        "Variational Auto Encoders (VAEs) can be thought of as what all but the last layer of a neural network is doing, namely feature extraction or seperating out the data. Thus given some data we can think of using a neural network for representation generation.\n",
        "\n",
        "Recall that the goal of a generative model is to estimate the probability distribution of high dimensional data such as images, videos, audio or even text by learning the underlying structure in the data as well as the dependencies between the different elements of the data. This is very useful since we can then use this representation to generate new data with similar properties. This way we can also learn useful features from the data in an unsupervised fashion.\n",
        "\n",
        "The VQ-VAE uses a discrete latent representation mostly because many important real-world objects are discrete. For example in images we might have categories like \"Cat\", \"Car\", etc. and it might not make sense to interpolate between these categories. Discrete representations are also easier to model since each category has a single value whereas if we had a continous latent space then we will need to normalize this density function and learn the dependencies between the different variables which could be very complex.\n",
        "\n",
        "### Code\n",
        "\n",
        "I have followed the code from the TensorFlow implementation by the author which you can find here [vqvae.py](https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/nets/vqvae.py) and [vqvae_example.ipynb](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/vqvae_example.ipynb).\n",
        "\n",
        "Another PyTorch implementation is found at [pytorch-vqvae](https://github.com/ritheshkumar95/pytorch-vqvae).\n",
        "\n",
        "\n",
        "### Basic Idea\n",
        "\n",
        "The overall architecture is summarized in the diagram below:\n",
        "\n",
        "![](https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/images/vq-vae.png?raw=1)\n",
        "\n",
        "We start by defining a latent embedding space of dimension `[K, D]` where `K` are the number of embeddings and `D` is the dimensionality of each latent embeddng vector, i.e. $e_i \\in \\mathbb{R}^{D}$. The model is comprised of an encoder and a decoder. The encoder will map the input to a sequence of discrete latent variables, whereas the decoder will try to reconstruct the input from these latent sequences.\n",
        "\n",
        "More preciesly, the model will take in batches of RGB images,  say $x$, each of size 32x32 for our example, and pass it through a ConvNet encoder producing some output $E(x)$, where we make sure the channels are the same as the dimensionality of the latent embedding vectors. To calculate the discrete latent variable we find the nearest embedding vector and output it's index.\n",
        "\n",
        "The input to the decoder is the embedding vector corresponding to the index which is passed through the decoder to produce the reconstructed image.\n",
        "\n",
        "Since the nearest neighbour lookup has no real gradient in the backward pass we simply pass the gradients from the decoder to the encoder  unaltered. The intuition is that since the output representation of the encoder and the input to the decoder share the same `D` channel dimensional space, the gradients contain useful information for how the encoder has to change its output to lower the reconstruction loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtAjemVWElum"
      },
      "source": [
        "### Vector Quantizer Layer\n",
        "\n",
        "This layer takes a tensor to be quantized. The channel dimension will be used as the space in which to quantize. All other dimensions will be flattened and will be seen as different examples to quantize.\n",
        "\n",
        "The output tensor will have the same shape as the input.\n",
        "\n",
        "As an example for a `BCHW` tensor of shape `[16, 64, 32, 32]`, we will first convert it to an `BHWC` tensor of shape `[16, 32, 32, 64]` and then reshape it into `[16384, 64]` and all `16384` vectors of size `64`  will be quantized independently. In otherwords, the channels are used as the space in which to quantize. All other dimensions will be flattened and be seen as different examples to quantize, `16384` in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51PMSjgtElum"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5PJ3e8ZElun"
      },
      "source": [
        "We will also implement a slightly modified version  which will use exponential moving averages to update the embedding vectors instead of an auxillary loss. This has the advantage that the embedding updates are independent of the choice of optimizer for the encoder, decoder and other parts of the architecture. For most experiments the EMA version trains faster than the non-EMA version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0F968t5WElun"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JASaaD6tElun"
      },
      "source": [
        "### Encoder & Decoder Architecture\n",
        "\n",
        "The encoder and decoder architecture is based on a ResNet and is implemented below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RogLy8BWElun"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "temAvXn1Eluo"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=num_hiddens//2,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1, padding=1)\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._conv_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._conv_3(x)\n",
        "        return self._residual_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DomC9GIdEluo"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1, padding=1)\n",
        "\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,\n",
        "                                                out_channels=num_hiddens//2,\n",
        "                                                kernel_size=4,\n",
        "                                                stride=2, padding=1)\n",
        "\n",
        "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n",
        "                                                out_channels=num_channels,\n",
        "                                                kernel_size=4,\n",
        "                                                stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "\n",
        "        x = self._residual_stack(x)\n",
        "\n",
        "        x = self._conv_trans_1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return self._conv_trans_2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kytMRnED2-0"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_channels, num_filters_last=64, n_layers=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(num_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
        "        num_filters_mult = 1\n",
        "\n",
        "        for i in range(1, n_layers + 1):\n",
        "            num_filters_mult_last = num_filters_mult\n",
        "            num_filters_mult = min(2 ** i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
        "                          2 if i < n_layers else 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0SAbtlvEluo"
      },
      "source": [
        "### Training Experiments\n",
        "\n",
        "We use the hyperparameters from the author's code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wsU0gQAEluo"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "decay = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AhOJdQuElup"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_channels, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(num_channels, num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n",
        "                                      out_channels=embedding_dim,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1)\n",
        "        if decay > 0.0:\n",
        "            print(f'Performing EMA updates with decay rate: {decay}...')\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                              commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
        "                                           commitment_cost)\n",
        "        self._decoder = Decoder(embedding_dim,\n",
        "                                num_channels,\n",
        "                                num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss, epsilon=1e-4, max_lambda=1e4, scale=0.8):\n",
        "        '''Calculate the lambda value for the loss function.\n",
        "        '''\n",
        "        ell = self._decoder._conv_trans_2 # the last layer of the decoder\n",
        "        ell_weight = ell.weight\n",
        "        perceptual_loss_gradients = torch.autograd.grad(perceptual_loss, ell_weight, retain_graph=True)[0]\n",
        "        gan_loss_gradients = torch.autograd.grad(gan_loss, ell_weight, retain_graph=True)[0]\n",
        "\n",
        "        lambda_factor = torch.norm(perceptual_loss_gradients) / torch.norm(gan_loss_gradients + epsilon)\n",
        "        lambda_factor = torch.clamp(lambda_factor, min=0.0, max=max_lambda).detach()\n",
        "\n",
        "        return scale * lambda_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x)\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        x_recon = self._decoder(quantized)\n",
        "\n",
        "        return loss, x_recon, perplexity, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y4TQnkFFIkF"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    \"\"\"Initialize the weights of the module.\n",
        "\n",
        "    Args:\n",
        "        m (nn.Module): Module to initialize the weights of.\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRDRUUYGELsb",
        "outputId": "d1eabd1e-e979-46d4-9868-a2edbdd96821"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (model): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "discriminator = Discriminator(num_channels=num_channels).to(device)\n",
        "discriminator.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0_owngAElup",
        "outputId": "37c80200-b668-4845-e9c5-cb8814e4c466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing EMA updates with decay rate: 0.99...\n",
            "==================================================\n",
            "Model Summary:\n",
            "Model(\n",
            "  (_encoder): Encoder(\n",
            "    (_conv_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (_conv_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (_conv_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (_residual_stack): ResidualStack(\n",
            "      (_layers): ModuleList(\n",
            "        (0-1): 2 x Residual(\n",
            "          (_block): Sequential(\n",
            "            (0): ReLU(inplace=True)\n",
            "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (2): ReLU(inplace=True)\n",
            "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (_pre_vq_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (_vq_vae): VectorQuantizerEMA(\n",
            "    (_embedding): Embedding(512, 16)\n",
            "  )\n",
            "  (_decoder): Decoder(\n",
            "    (_conv_1): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (_residual_stack): ResidualStack(\n",
            "      (_layers): ModuleList(\n",
            "        (0-1): 2 x Residual(\n",
            "          (_block): Sequential(\n",
            "            (0): ReLU(inplace=True)\n",
            "            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "            (2): ReLU(inplace=True)\n",
            "            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (_conv_trans_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (_conv_trans_2): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "--------------------------------------------------\n",
            "Experiment Settings:\n",
            "- Number of epochs: 1000\n",
            "- Learning rate: 0.0001\n",
            "- Batch size: 8\n",
            "- Commitment cost: 0.25\n",
            "- Number of Embeddings: 512\n",
            "- Embedding dimension: 16\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1000: 100%|██████████| 6250/6250 [02:07<00:00, 49.04batch/s, gan_loss=0, perplexity=118, psnr=10.8, recon_error=0.0849, vq_loss=0.0153]\n",
            "Epoch 2/1000:  61%|██████    | 3789/6250 [01:16<00:47, 51.62batch/s, gan_loss=0, perplexity=170, psnr=11.4, recon_error=0.0735, vq_loss=0.0235]"
          ]
        }
      ],
      "source": [
        "model = Model(num_hiddens, num_channels, num_residual_layers, num_residual_hiddens,\n",
        "              num_embeddings, embedding_dim,\n",
        "              commitment_cost, decay).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
        "opt_disc = optim.Adam(discriminator.parameters(), lr=learning_rate, amsgrad=False)\n",
        "\n",
        "print('='*50)\n",
        "print('Model Summary:')\n",
        "print(model.eval())\n",
        "print('-'*50)\n",
        "print('Experiment Settings:')\n",
        "print(f'- Number of epochs: {num_epochs}')\n",
        "print(f'- Learning rate: {learning_rate}')\n",
        "print(f'- Batch size: {batch_size}')\n",
        "print(f'- Commitment cost: {commitment_cost}')\n",
        "print(f'- Number of Embeddings: {num_embeddings}')\n",
        "print(f'- Embedding dimension: {embedding_dim}')\n",
        "print('='*50)\n",
        "\n",
        "model.train()\n",
        "train_res_recon_error = []\n",
        "train_res_perplexity = []\n",
        "train_res_vq_loss = []\n",
        "train_res_gan_loss = []\n",
        "train_res_psnr = []\n",
        "disc_factor = 0.\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  with tqdm(training_loader, unit=\"batch\") as pbar:\n",
        "      for data, _ in pbar:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vq_loss, data_recon, perplexity, latents = model(data)\n",
        "\n",
        "        recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "        loss = recon_error + vq_loss\n",
        "\n",
        "        opt_disc.zero_grad()\n",
        "\n",
        "        disc_real = discriminator(data)\n",
        "        disc_fake = discriminator(data_recon)\n",
        "\n",
        "        disc_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "        disc_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "\n",
        "        if global_step > disc_start:\n",
        "          disc_factor = 0.2\n",
        "\n",
        "        g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "        loss += disc_factor * model.calculate_lambda(recon_error, g_loss) * g_loss\n",
        "\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        gan_loss = disc_factor * 0.5 * (disc_loss_real + disc_loss_fake)\n",
        "\n",
        "        gan_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        opt_disc.step()\n",
        "\n",
        "        # compute PSNR\n",
        "        psnr = 10 * torch.log10(1 / recon_error)\n",
        "\n",
        "        train_res_recon_error.append(recon_error.item())\n",
        "        train_res_perplexity.append(perplexity.item())\n",
        "        train_res_vq_loss.append(vq_loss.item())\n",
        "        train_res_gan_loss.append(gan_loss.item())\n",
        "        train_res_psnr.append(psnr.item())\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        pbar.set_postfix(recon_error=np.mean(train_res_recon_error[-100:]),\n",
        "                          perplexity=np.mean(train_res_perplexity[-100:]),\n",
        "                          vq_loss=np.mean(train_res_vq_loss[-100:]),\n",
        "                          gan_loss=np.mean(train_res_gan_loss[-100:]),\n",
        "                          psnr=np.mean(train_res_psnr[-100:]))\n",
        "\n",
        "        global_step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53eWlQ9FEqEZ"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aug55hWsDoh8"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'vqgan.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5llYww18FsYT"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in model._encoder.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZKa4woBElup"
      },
      "source": [
        "### Plot Evaluations for the VQ-GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs9ccAMwElup"
      },
      "outputs": [],
      "source": [
        "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
        "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)\n",
        "train_res_vq_loss_smooth = savgol_filter(train_res_vq_loss, 201, 7)\n",
        "# train_res_gan_loss_smooth = savgol_filter(train_res_gan_loss, 201, 7)\n",
        "train_res_psnr_smooth = savgol_filter(train_res_psnr, 201, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwdJMiOWElup"
      },
      "outputs": [],
      "source": [
        "f = plt.figure(figsize=(16,10))\n",
        "ax = f.add_subplot(2,2,1)\n",
        "ax.plot(train_res_recon_error_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed NMSE.')\n",
        "ax.set_xlabel('iteration')\n",
        "ax = f.add_subplot(2,2,2)\n",
        "ax.plot(train_res_perplexity_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed perplexity.')\n",
        "ax.set_xlabel('iteration')\n",
        "ax = f.add_subplot(2,2,3)\n",
        "ax.plot(train_res_vq_loss_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed VQ loss.')\n",
        "ax.set_xlabel('iteration')\n",
        "ax = f.add_subplot(2,2,4)\n",
        "# ax.plot(train_res_gan_loss_smooth)\n",
        "# ax.set_yscale('log')\n",
        "# ax.set_title('Smoothed GAN loss.')\n",
        "# ax.set_xlabel('iteration')\n",
        "ax.plot(train_res_psnr_smooth)\n",
        "ax.set_title('Smoothed PSNR.')\n",
        "ax.set_xlabel('iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00av3gZtEluq"
      },
      "source": [
        "### View Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFvvup7IEluq"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "(valid_originals, _) = next(iter(validation_loader))\n",
        "valid_originals = valid_originals.to(device)\n",
        "\n",
        "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
        "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
        "valid_reconstructions = model._decoder(valid_quantize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTIOeVSqEluq"
      },
      "outputs": [],
      "source": [
        "(train_originals, _) = next(iter(training_loader))\n",
        "train_originals = train_originals.to(device)\n",
        "_, train_reconstructions, _, _ = model._vq_vae(train_originals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JGKn04gxWVx"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (15, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-cuGXUFEluq"
      },
      "outputs": [],
      "source": [
        "def show(img):\n",
        "    npimg = img.numpy()\n",
        "    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest', cmap='gray')\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgelmgslJF4P"
      },
      "outputs": [],
      "source": [
        "channel_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-_IMZmM0DEU"
      },
      "outputs": [],
      "source": [
        "# Latent space images\n",
        "latents = vq_output_eval.permute(0, 2, 3, 1).contiguous().view(-1, embedding_dim)\n",
        "U, S, V = torch.pca_lowrank(latents)\n",
        "projections = torch.matmul(latents, V[:, :num_channels]).view(32, num_channels, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs = projections.cpu().data\n",
        "for i in range(len(latent_imgs)):\n",
        "    for j in range(num_channels):\n",
        "      tmp = latent_imgs[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kg77ph-JF4P"
      },
      "outputs": [],
      "source": [
        "projections_top = torch.matmul(latents, V[:, channel_idx]).view(32, 1, 7, 7) # project to 1 channel to view the latents as RGB images\n",
        "latent_imgs_top = projections_top.cpu().data\n",
        "for i in range(len(latent_imgs_top)):\n",
        "    for j in range(1):\n",
        "      tmp = latent_imgs_top[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs_top[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs_top), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qv6ftShGnyF"
      },
      "outputs": [],
      "source": [
        "# Latent space reconstructions\n",
        "reconstructed_latents = valid_quantize.permute(0, 2, 3, 1).contiguous().view(-1, embedding_dim)\n",
        "U, S, V = torch.pca_lowrank(reconstructed_latents)\n",
        "projections = torch.matmul(reconstructed_latents, V[:, :num_channels]).view(32, num_channels, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs = projections.cpu().data\n",
        "for i in range(len(latent_imgs)):\n",
        "    for j in range(num_channels):\n",
        "      tmp = latent_imgs[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGMi6RVgJF4P"
      },
      "outputs": [],
      "source": [
        "projections_recon_top = torch.matmul(reconstructed_latents, V[:, channel_idx]).view(32, 1, 7, 7) # project to 1 channel to view the latents as RGB images\n",
        "latent_imgs_recon_top = projections_recon_top.cpu().data\n",
        "for i in range(len(latent_imgs_recon_top)):\n",
        "    for j in range(1):\n",
        "      tmp = latent_imgs_recon_top[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs_recon_top[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs_recon_top), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyFWe5stGzKJ"
      },
      "outputs": [],
      "source": [
        "print(F.mse_loss(latents, reconstructed_latents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMJppVQPEluq"
      },
      "outputs": [],
      "source": [
        "show(make_grid(valid_originals.cpu()+0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxid-PKmZuFz"
      },
      "outputs": [],
      "source": [
        "reconstructed_imgs = valid_reconstructions.cpu().data + 0.5\n",
        "\n",
        "for i in range(len(reconstructed_imgs)):\n",
        "    for j in range(1):\n",
        "      tmp = torch.clamp(reconstructed_imgs[i,j,:,:], 0, 1)\n",
        "      reconstructed_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(reconstructed_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Qsz-coJF4P"
      },
      "outputs": [],
      "source": [
        "# compute PSNR\n",
        "mse = F.mse_loss(valid_originals, valid_reconstructions)\n",
        "psnr = 10 * torch.log10(1 / mse)\n",
        "print(f'PSNR: {psnr}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSnb7dvsElur"
      },
      "source": [
        "### View Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDdNnMLTElur"
      },
      "outputs": [],
      "source": [
        "proj = umap.UMAP(n_neighbors=3,\n",
        "                 min_dist=0.1,\n",
        "                 metric='cosine').fit_transform(model._vq_vae._embedding.weight.data.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kvweEYUElur"
      },
      "outputs": [],
      "source": [
        "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX4lPv3OJF4Q"
      },
      "source": [
        "### Visualize the Codebook Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wz7BcHFJF4Q"
      },
      "outputs": [],
      "source": [
        "codebook = model._vq_vae._embedding.weight.data\n",
        "print(codebook.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWoDMR36JF4Q"
      },
      "outputs": [],
      "source": [
        "def visualize_codebook(codebook, nrows, ncols):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    assert nrows * ncols <= codebook.shape[0]\n",
        "\n",
        "    for i in range(nrows * ncols):\n",
        "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
        "        embedding_vector = codebook[i].view(4, 4, 1).contiguous().cpu().data # convert to RGB format\n",
        "        embedding_vector -= embedding_vector.min()\n",
        "        embedding_vector /= embedding_vector.max()\n",
        "        ax.imshow(embedding_vector.numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WqviPaUJF4Q"
      },
      "outputs": [],
      "source": [
        "visualize_codebook(codebook, 16, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv-XSOJgTGF3"
      },
      "source": [
        "## DL-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjn4JLywRZzu"
      },
      "source": [
        "### Dictionary Learning Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVlvgsUoElur"
      },
      "outputs": [],
      "source": [
        "class DictLearn(nn.Module):\n",
        "    \"\"\"\n",
        "    Dictionary Learning.\n",
        "\n",
        "    See:\n",
        "    - M. Aharon, M. Elad, and A. Bruckstein, \"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation,\" IEEE Trans. Signal Processing, vol. 54, no. 11, pp. 4311-4322, 2006.\n",
        "    - Rubinstein, R., Zibulevsky, M. and Elad, M., \"Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit,\" CS Technion, 2008.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, eta, sparsity_level):\n",
        "        \"\"\"\n",
        "        class constructor for DictLearn\n",
        "\n",
        "        :param embedding_dim: dimension of the embedding\n",
        "        :param num_embeddings: number of dictionary atoms\n",
        "        :param sparsity_level: maximum sparsity (number of non-zero coefficients) of the representation, reduces to K-Means (Vector Quantization) when set to 1\n",
        "        :param initial_dict: initial dictionary if given, otherwise random rows from the data matrix are used\n",
        "        :param max_iter: maximum number of iterations\n",
        "        \"\"\"\n",
        "        super(DictLearn, self).__init__()\n",
        "\n",
        "        self._num_embeddings = num_embeddings\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._commitment_cost = commitment_cost\n",
        "        self._eta = eta\n",
        "        self._sparsity_level = sparsity_level\n",
        "\n",
        "        self._dictionary = nn.Embedding(embedding_dim, num_embeddings)\n",
        "        self._dictionary.weight.data.normal_(0, 1)\n",
        "        self._dictionary.weight.data.copy_(self._dictionary.weight / torch.linalg.norm(self._dictionary.weight, dim=0))\n",
        "\n",
        "        self._gamma = None\n",
        "        self._A = None\n",
        "        self._B = None\n",
        "\n",
        "    def forward(self, z_e):\n",
        "        if z_e.shape[2] * z_e.shape[3] < self._num_embeddings:\n",
        "            kernel_size = z_e.shape[2]\n",
        "        else:\n",
        "            kernel_size = 16\n",
        "\n",
        "        stride = kernel_size # non-overlapping patches\n",
        "\n",
        "        # break the input tensor into patches\n",
        "        patches = F.unfold(z_e, kernel_size=kernel_size, stride=stride).permute(2, 0, 1).contiguous()\n",
        "        patches_shape = patches.shape\n",
        "        patches = patches.view(patches.shape[0] * patches.shape[1], self._embedding_dim, kernel_size, kernel_size).contiguous()\n",
        "\n",
        "        # permute\n",
        "        z_e = z_e.permute(0, 2, 3, 1).contiguous()\n",
        "        ze_shape = z_e.shape\n",
        "        # Flatten input\n",
        "        ze_flattened = z_e.view(self._embedding_dim, -1).contiguous() # convert to column-major order, i.e., each column is a data point\n",
        "\n",
        "        # flatten patches\n",
        "        patches = patches.view(self._embedding_dim, -1).contiguous()\n",
        "\n",
        "        \"\"\"\n",
        "        Sparse Coding Stage\n",
        "        \"\"\"\n",
        "\n",
        "        if self._gamma is None:\n",
        "          # initialize dictionary with random columns of z_e\n",
        "          # self._dictionary.weight.data.copy_(nn.Parameter(z_e[:, torch.randperm(z_e.shape[1])[:self._num_embeddings]]))\n",
        "          # normalize the dictionary\n",
        "          self._dictionary.weight.data.copy_(nn.Parameter(self._dictionary.weight / torch.linalg.norm(self._dictionary.weight, dim=0)))\n",
        "          # initialize the sparse codes\n",
        "          self._gamma = nn.Parameter(self.update_gamma(patches.detach(), self._dictionary.weight.detach(), debug=False))\n",
        "        else:\n",
        "          # normalize dictionary\n",
        "          self._dictionary.weight.data.copy_(nn.Parameter(self._dictionary.weight / torch.linalg.norm(self._dictionary.weight, dim=0)))\n",
        "          # update the sparse codes\n",
        "          self._gamma.data.copy_(self.update_gamma(patches.detach(), self._dictionary.weight.detach(), debug=False))\n",
        "\n",
        "        encodings = self._gamma.detach() # sparse codes\n",
        "\n",
        "        # compute reconstruction\n",
        "        recon = self._dictionary.weight @ self._gamma.detach()\n",
        "        recon = recon.view(patches_shape).permute(1, 2, 0).contiguous() # convert to patches\n",
        "\n",
        "        # fold back the patches\n",
        "        recon = F.fold(recon, (ze_shape[1], ze_shape[2]), kernel_size=kernel_size, stride=stride).permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # compute loss\n",
        "        e_latent_loss = F.mse_loss(recon.detach(), z_e) * self._commitment_cost # latent loss from encoder\n",
        "        loss = e_latent_loss + F.mse_loss(recon, z_e.detach()) # reconstruction loss\n",
        "\n",
        "        # straight-through gradient estimator\n",
        "        recon = z_e + (recon - z_e).detach()\n",
        "\n",
        "        # compute perplexity\n",
        "        avg_probs = torch.mean(encodings.bool().float() / self._sparsity_level, dim=1) # convert nonzero entries of encodings to 1.0\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return loss, e_latent_loss, recon.permute(0, 3, 1, 2).contiguous(), z_e.detach(), perplexity, encodings\n",
        "\n",
        "    def update_dictionary(self, z_e, dictionary, t):\n",
        "        \"\"\"online dictionary update via Block Coordinate Descent.\n",
        "\n",
        "        References:\n",
        "        - Mairal J, Bach F, Ponce J, Sapiro G. Online dictionary learning for sparse coding.\n",
        "          In Proceedings of the 26th annual international conference on machine learning 2009 Jun 14 (pp. 689-696).\n",
        "        \"\"\"\n",
        "        # # compute beta\n",
        "        # theta = 0\n",
        "        # eta = z_e.shape[1]\n",
        "\n",
        "        # if t < eta:\n",
        "        #   theta = t * eta\n",
        "        # else:\n",
        "        #   theta = eta ** 2 + t - eta\n",
        "\n",
        "        # beta = (theta + 1 - eta) / (theta + 1)\n",
        "\n",
        "        # # precomputations\n",
        "        # if self._A is None:\n",
        "        #   self._A = self._gamma.mm(self._gamma.t()) + torch.diag(torch.ones(self._num_embeddings, device='cuda')) * 1e-10\n",
        "        # else:\n",
        "        #   self._A = nn.Parameter(beta * self._A + self._gamma.mm(self._gamma.t()) + torch.diag(torch.ones(self._num_embeddings, device='cuda')) * 1e-10)\n",
        "        # if self._B is None:\n",
        "        #   self._B = z_e.mm(self._gamma.t()) + torch.diag(torch.ones(self._embedding_dim, device='cuda')) * 1e-10\n",
        "        # else:\n",
        "        #   self._B = nn.Parameter(beta * self._B + z_e.mm(self._gamma.t()) + torch.diag(torch.ones(self._embedding_dim, device='cuda')) * 1e-10)\n",
        "\n",
        "        # self._dictionary.weight.data.copy_((self._B - self._dictionary.weight @ self._A) / self._A.diag() + self._dictionary.weight)\n",
        "        # self._dictionary.weight.data.copy_(self._dictionary.weight / torch.linalg.norm(self._dictionary.weight, dim=0))\n",
        "\n",
        "        self._dictionary.weight.data.copy_(\n",
        "            self._dictionary.weight - (self._eta / z_e.shape[1]) * (self._dictionary.weight @ self._gamma - z_e) @ torch.sign(self._gamma).t())\n",
        "        self._dictionary.weight.data.copy_(self._dictionary.weight / torch.linalg.norm(self._dictionary.weight, dim=0))\n",
        "\n",
        "    def update_gamma(self, signals, dictionary, debug=False):\n",
        "        \"\"\"sparse coding stage\n",
        "\n",
        "        Implemented using the Batch Orthogonal Matching Pursuit (OMP) algorithm.\n",
        "\n",
        "        Reference:\n",
        "        - Rubinstein, R., Zibulevsky, M. and Elad, M., \"Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit,\" CS Technion, 2008.\n",
        "\n",
        "        :param signals: input signals to be sparsely coded\n",
        "        \"\"\"\n",
        "        embedding_dim, num_signals = signals.shape\n",
        "        dictionary_t = dictionary.t() # save the transpose of the dictionary for faster computation\n",
        "        gram_matrix = dictionary_t.mm(dictionary) # the Gram matrix, dimension: num_atoms x num_atoms\n",
        "        eps = torch.norm(signals, dim=0) # residual, initialized as the L2 norm of the signal\n",
        "        corr_init = dictionary_t.mm(signals).t() # initial correlation vector, transposed to make num_signals the first dimension\n",
        "        gamma = torch.zeros_like(corr_init) # placeholder for the sparse coefficients\n",
        "\n",
        "        corr = corr_init\n",
        "        L = torch.ones(num_signals, 1, 1, device=signals.device) # contains the progressive Cholesky of the Gram matrix in the selected indices\n",
        "        I = torch.zeros(num_signals, 0, dtype=torch.long, device=signals.device) # placeholder for the index set\n",
        "        omega = torch.ones_like(corr_init, dtype=torch.bool) # used to zero out elements in corr before argmax\n",
        "        signal_idx = torch.arange(num_signals, device=signals.device)\n",
        "        delta = torch.zeros(num_signals, device=signals.device) # to track residuals\n",
        "\n",
        "        k = 0\n",
        "        while k < self._sparsity_level:\n",
        "            k += 1\n",
        "            k_hats = torch.argmax(torch.abs(corr * omega), dim=1) # select the index of the maximum correlation\n",
        "            # update omega to make sure we do not select the same index twice\n",
        "            omega[torch.arange(k_hats.shape[0], device=signals.device), k_hats] = 0\n",
        "            expanded_signal_idx = signal_idx.unsqueeze(0).expand(k, num_signals).t() # expand is more efficient than repeat\n",
        "\n",
        "            if k > 1: # Cholesky update\n",
        "                G_ = gram_matrix[I[signal_idx, :], k_hats[expanded_signal_idx[...,:-1]]].view(num_signals, k - 1, 1) # compute for all signals in a vectorized manner\n",
        "                w = torch.linalg.solve_triangular(L, G_, upper=False).view(-1, 1, k - 1)\n",
        "                w_br = torch.sqrt(1 - (w**2).sum(dim=2, keepdim=True)) # L bottom-right corner element: sqrt(1 - w.t().mm(w))\n",
        "\n",
        "                # concatenate into the new Cholesky: L <- [[L, 0], [w, w_br]]\n",
        "                k_zeros = torch.zeros(num_signals, k - 1, 1, device=signals.device)\n",
        "                L = torch.cat((\n",
        "                    torch.cat((L, k_zeros), dim=2),\n",
        "                    torch.cat((w, w_br), dim=2),\n",
        "                    ), dim=1)\n",
        "\n",
        "            # update non-zero indices\n",
        "            I = torch.cat([I, k_hats.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # solve L\n",
        "            corr_ = corr_init[expanded_signal_idx, I[signal_idx, :]].view(num_signals, k, 1)\n",
        "            gamma_ = torch.cholesky_solve(corr_, L)\n",
        "\n",
        "            # de-stack gamma into the non-zero elements\n",
        "            gamma[signal_idx.unsqueeze(1), I[signal_idx]] = gamma_[signal_idx].squeeze(-1)\n",
        "\n",
        "            # beta = G_I * gamma_I\n",
        "            beta = gamma[signal_idx.unsqueeze(1), I[signal_idx]].unsqueeze(1).bmm(gram_matrix[I[signal_idx], :]).squeeze(1)\n",
        "\n",
        "            corr = corr_init - beta\n",
        "\n",
        "            # update residual\n",
        "            # new_delta = (gamma * beta).sum(dim=1)\n",
        "            # eps += delta-new_delta\n",
        "            # delta = new_delta\n",
        "\n",
        "            if debug and k % 1 == 0:\n",
        "              print('Step {}, residual: {:.4f}, below tolerance: {:.4f}'.format(k, eps.max(), (eps < 1e-7).float().mean().item()))\n",
        "\n",
        "        return gamma.t() # transpose the sparse coefficients to make num_signals the first dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNm0gM5bIvTL"
      },
      "source": [
        "### Define the Model DL-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be_CFh-yIz8J"
      },
      "outputs": [],
      "source": [
        "class DLGAN(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_channels, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, eta, sparsity_level):\n",
        "        super(DLGAN, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(num_channels, num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n",
        "                                      out_channels=embedding_dim,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1)\n",
        "\n",
        "        self._dl_bottleneck = DictLearn(num_embeddings,\n",
        "                                     embedding_dim,\n",
        "                                     commitment_cost=commitment_cost,\n",
        "                                     eta=eta,\n",
        "                                     sparsity_level=sparsity_level)\n",
        "\n",
        "        self._decoder = Decoder(embedding_dim,\n",
        "                                num_channels,\n",
        "                                num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x, global_step):\n",
        "        z = self._encoder(x)\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, dl_loss, sparsified, latents, perplexity, encodings = self._dl_bottleneck(z)\n",
        "        # self._dl_bottleneck.update_dictionary(latents.detach(), self._dl_bottleneck._dictionary.weight, global_step)\n",
        "        x_recon = self._decoder(sparsified)\n",
        "\n",
        "        return loss, dl_loss, x_recon, latents, perplexity, encodings\n",
        "\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss, epsilon=1e-4, max_lambda=1e4, scale=0.8):\n",
        "        '''Calculate the lambda value for the loss function.\n",
        "        '''\n",
        "        ell = self._decoder._conv_trans_2 # the last layer of the decoder\n",
        "        ell_weight = ell.weight\n",
        "        perceptual_loss_gradients = torch.autograd.grad(perceptual_loss, ell_weight, retain_graph=True)[0]\n",
        "        gan_loss_gradients = torch.autograd.grad(gan_loss, ell_weight, retain_graph=True)[0]\n",
        "\n",
        "        lambda_factor = torch.norm(perceptual_loss_gradients) / torch.norm(gan_loss_gradients + epsilon)\n",
        "        lambda_factor = torch.clamp(lambda_factor, min=0.0, max=max_lambda).detach()\n",
        "\n",
        "        return scale * lambda_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_9UvC9d_Kfb"
      },
      "outputs": [],
      "source": [
        "dl_disc = Discriminator(num_channels).to(device)\n",
        "dl_disc.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD0betSRTVVq"
      },
      "source": [
        "### Training Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADf4mmfuJ6K5"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "sparsity_level = 8\n",
        "eta = int(embedding_dim / sparsity_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeR7ymxqJpjC"
      },
      "outputs": [],
      "source": [
        "dlgan_model = DLGAN(num_hiddens, num_channels, num_residual_layers, num_residual_hiddens,\n",
        "              num_embeddings, embedding_dim,\n",
        "              commitment_cost, eta, sparsity_level).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNZZ9EKpESyC"
      },
      "outputs": [],
      "source": [
        "dlgan_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPwBW4fVNDKS"
      },
      "outputs": [],
      "source": [
        "# optimizer setup\n",
        "optimizer = optim.Adam(dlgan_model.parameters(), lr=learning_rate, amsgrad=False)\n",
        "opt_disc = optim.Adam(dl_disc.parameters(), lr=learning_rate, amsgrad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSNtHF5FKOIJ"
      },
      "outputs": [],
      "source": [
        "dlgan_model.train()\n",
        "\n",
        "print('='*50)\n",
        "print('Experiment Settings:')\n",
        "print(f'- Number of epochs: {num_epochs}')\n",
        "print(f'- Learning rate: {learning_rate}')\n",
        "print(f'- Batch size: {batch_size}')\n",
        "print(f'- Sparsity level: {sparsity_level}')\n",
        "print(f'- eta: {eta}')\n",
        "print(f'- Commitment cost: {commitment_cost}')\n",
        "print(f'- Number of Embeddings: {num_embeddings}')\n",
        "print(f'- Embedding dimension: {embedding_dim}')\n",
        "print('='*50)\n",
        "train_res_recon_error = []\n",
        "train_res_perplexity = []\n",
        "train_res_dl_loss = []\n",
        "train_res_gan_loss = []\n",
        "train_res_psnr = []\n",
        "disc_factor = 0.\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  with tqdm(training_loader, unit=\"batch\") as pbar:\n",
        "    for i, (data, _) in enumerate(pbar):\n",
        "      (data, _) = next(iter(training_loader))\n",
        "      data = data.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      sc_loss, dl_loss, data_recon, latents, perplexity, encodings = dlgan_model(data, global_step)\n",
        "\n",
        "      if debug:\n",
        "        print(f'DEBUG: sparsity check: {torch.count_nonzero(encodings, dim=0)}')\n",
        "\n",
        "      recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "\n",
        "      loss = sc_loss + recon_error\n",
        "\n",
        "      opt_disc.zero_grad()\n",
        "\n",
        "      disc_real = discriminator(data)\n",
        "      disc_fake = discriminator(data_recon)\n",
        "\n",
        "      disc_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "      disc_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "\n",
        "      if global_step > disc_start:\n",
        "        disc_factor = 0.2\n",
        "\n",
        "      g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "      loss += disc_factor * dlgan_model.calculate_lambda(recon_error, g_loss) * g_loss\n",
        "\n",
        "      loss.backward(retain_graph=True)\n",
        "\n",
        "      gan_loss = disc_factor * 0.5 * (disc_loss_real + disc_loss_fake)\n",
        "\n",
        "      gan_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      opt_disc.step()\n",
        "\n",
        "      # compute PSNR\n",
        "      psnr = 10 * torch.log10(1 / recon_error)\n",
        "\n",
        "      train_res_recon_error.append(recon_error.item())\n",
        "      train_res_perplexity.append(perplexity.item())\n",
        "      train_res_dl_loss.append(dl_loss.item())\n",
        "      train_res_gan_loss.append(gan_loss.item())\n",
        "      train_res_psnr.append(psnr.item())\n",
        "\n",
        "      global_step += 1\n",
        "\n",
        "      pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "      pbar.set_postfix(recon_error=np.mean(train_res_recon_error[-100:]),\n",
        "                       perplexity=np.mean(train_res_perplexity[-100:]),\n",
        "                       dl_loss=np.mean(train_res_dl_loss[-100:]),\n",
        "                       gan_loss=np.mean(train_res_gan_loss[-100:]),\n",
        "                       psnr=np.mean(train_res_psnr[-100:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyMeTkyHDhiM"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "torch.save(dlgan_model.state_dict(), 'dlgan.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vwR-pekFSCL"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in dlgan_model._encoder.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KV74HBESFVm"
      },
      "source": [
        "### Plot Evaluations for the DL-GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHdZHTPlSIwe"
      },
      "outputs": [],
      "source": [
        "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
        "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 2)\n",
        "train_res_dl_loss_smooth = savgol_filter(train_res_dl_loss, 201, 7)\n",
        "# train_res_gan_loss_smooth = savgol_filter(train_res_gan_loss, 201, 7)\n",
        "train_res_psnr_smooth = savgol_filter(train_res_psnr, 201, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fLpxMdKSaAB"
      },
      "outputs": [],
      "source": [
        "f = plt.figure(figsize=(16,10))\n",
        "ax = f.add_subplot(2,2,1)\n",
        "ax.plot(train_res_recon_error_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed NMSE.')\n",
        "ax.set_xlabel('Iteration')\n",
        "ax = f.add_subplot(2,2,2)\n",
        "ax.plot(train_res_perplexity_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed Perplexity.')\n",
        "ax.set_xlabel('Iteration')\n",
        "ax = f.add_subplot(2,2,3)\n",
        "ax.plot(train_res_dl_loss_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('Smoothed DL loss.')\n",
        "ax.set_xlabel('Iteration')\n",
        "ax = f.add_subplot(2,2,4)\n",
        "# ax.plot(train_res_gan_loss_smooth)\n",
        "# ax.set_yscale('log')\n",
        "# ax.set_title('Smoothed GAN loss.')\n",
        "# ax.set_xlabel('Iteration')\n",
        "ax.plot(train_res_psnr_smooth)\n",
        "ax.set_title('Smoothed PSNR.')\n",
        "ax.set_xlabel('Iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HEWxsscSn5z"
      },
      "source": [
        "### View Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pf7KjJ1Spoz"
      },
      "outputs": [],
      "source": [
        "dlgan_model.eval()\n",
        "\n",
        "(valid_originals, _) = next(iter(validation_loader))\n",
        "valid_originals = valid_originals.to(device)\n",
        "\n",
        "sc_output_eval = dlgan_model._pre_vq_conv(dlgan_model._encoder(valid_originals))\n",
        "# preprocess\n",
        "sc_output_eval = sc_output_eval.permute(0, 2, 3, 1).contiguous() # convert to column-major order, i.e., each column is a data point\n",
        "sc_output_eval_shape = sc_output_eval.shape\n",
        "sc_output_eval = sc_output_eval.view(embedding_dim, -1).contiguous() # convert to column-major order, i.e., each column is a data point\n",
        "# normalize the dictionary\n",
        "dictionary = dlgan_model._dl_bottleneck._dictionary.weight.data / torch.linalg.norm(dlgan_model._dl_bottleneck._dictionary.weight.data, dim=0)\n",
        "# compute the sparse code\n",
        "valid_sc = dlgan_model._dl_bottleneck.update_gamma(sc_output_eval, dictionary, debug=False)\n",
        "# reconstruct the latent space\n",
        "valid_latent = dictionary @ valid_sc\n",
        "valid_latent = valid_latent.view(sc_output_eval_shape).permute(0, 3, 1, 2).contiguous()\n",
        "# reconstruct the images\n",
        "valid_reconstructions = dlgan_model._decoder(valid_latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5mRZskFTfRz"
      },
      "outputs": [],
      "source": [
        "show(make_grid(valid_originals.cpu() + 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF8DwbeaTn3c"
      },
      "outputs": [],
      "source": [
        "reconstructed_imgs = valid_reconstructions.cpu().data + 0.5\n",
        "\n",
        "for i in range(len(reconstructed_imgs)):\n",
        "    for j in range(num_channels):\n",
        "      tmp = torch.clamp(reconstructed_imgs[i,j,:,:], 0, 1)\n",
        "      reconstructed_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(reconstructed_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvbynhy5JF4R"
      },
      "outputs": [],
      "source": [
        "# compute psnr\n",
        "recon_error = F.mse_loss(valid_reconstructions, valid_originals)\n",
        "psnr = 10 * torch.log10(1 / recon_error)\n",
        "print(f'PSNR: {psnr.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRrNgue-TN06"
      },
      "source": [
        "### View Latent Space Reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5viT6-PXJF4R"
      },
      "outputs": [],
      "source": [
        "channel_idx = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqlqwFPGVQ8b"
      },
      "outputs": [],
      "source": [
        "# Latent space images\n",
        "original_latents = sc_output_eval.view(-1, embedding_dim)\n",
        "original_latents_shape = original_latents.shape\n",
        "U, S, V = torch.pca_lowrank(original_latents)\n",
        "projections = torch.matmul(original_latents, V[:, :num_channels]).view(32, num_channels, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs = projections.cpu().data\n",
        "for i in range(len(latent_imgs)):\n",
        "    for j in range(num_channels):\n",
        "      tmp = latent_imgs[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xaq5-oI0JF4R"
      },
      "outputs": [],
      "source": [
        "# Latent space images top channel\n",
        "projections_top = torch.matmul(original_latents, V[:, channel_idx]).view(32, 1, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs_top = projections_top.cpu().data\n",
        "for i in range(len(latent_imgs_top)):\n",
        "    for j in range(1):\n",
        "      tmp = latent_imgs_top[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs_top[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs_top), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OamySbeSTPn1"
      },
      "outputs": [],
      "source": [
        "# Latent space images\n",
        "reconstructed_latents = valid_latent.permute(0, 2, 3, 1).contiguous().view(-1, embedding_dim)\n",
        "U, S, V = torch.pca_lowrank(reconstructed_latents)\n",
        "projections = torch.matmul(reconstructed_latents, V[:, :3]).view(32, num_channels, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs = projections.cpu().data\n",
        "for i in range(len(latent_imgs)):\n",
        "    for j in range(num_channels):\n",
        "      tmp = latent_imgs[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7adrljVlJF4S"
      },
      "outputs": [],
      "source": [
        "# Latent space images\n",
        "projections_recon_top = torch.matmul(reconstructed_latents, V[:, channel_idx]).view(32, 1, 7, 7) # project to 3 channel to view the latents as RGB images\n",
        "latent_imgs_recon_top = projections_recon_top.cpu().data\n",
        "for i in range(len(latent_imgs_recon_top)):\n",
        "    for j in range(1):\n",
        "      tmp = latent_imgs_recon_top[i,j,:,:]\n",
        "      tmp -= tmp.min()\n",
        "      tmp /= tmp.max()\n",
        "      latent_imgs_recon_top[i,j,:,:] = tmp\n",
        "show(make_grid(latent_imgs_recon_top), )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FQwfBMpGUVd"
      },
      "outputs": [],
      "source": [
        "print(F.mse_loss(original_latents, reconstructed_latents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE1t6R2tWHSF"
      },
      "source": [
        "### View the Dictionary Atoms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yaut2yynrRDy"
      },
      "outputs": [],
      "source": [
        "dictionary = dlgan_model._dl_bottleneck._dictionary.weight.t().data # dim: K x D\n",
        "dictionary.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4989Zjj7WK4_"
      },
      "outputs": [],
      "source": [
        "proj = umap.UMAP(n_neighbors=3,\n",
        "                 min_dist=0.1,\n",
        "                 metric='cosine').fit_transform(dictionary.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvLb4sWcWRqM"
      },
      "outputs": [],
      "source": [
        "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ah29MufucE6"
      },
      "outputs": [],
      "source": [
        "def visualize_dictionary(dictionary, nrows, ncols):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    assert nrows * ncols <= dictionary.shape[0]\n",
        "\n",
        "    for i in range(nrows * ncols):\n",
        "        ax = fig.add_subplot(nrows, ncols, i + 1)\n",
        "        atom = dictionary[i].view(4, 4, 1).contiguous().cpu().data # convert to RGB format\n",
        "        atom -= atom.min()\n",
        "        atom /= atom.max()\n",
        "        ax.imshow(atom.numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZBe8YJivcLJ"
      },
      "outputs": [],
      "source": [
        "visualize_dictionary(dictionary, 16, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kAeHAimHiSZ"
      },
      "outputs": [],
      "source": [
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrpWF4TQJF4S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}